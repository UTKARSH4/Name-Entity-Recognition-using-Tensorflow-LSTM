{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data\n",
    "\n",
    "Let's prepare data for our model. The dataset contains sentence and tags (for the respective sentence) columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from evaluation import *\n",
    "import pandas as pd\n",
    "import random\n",
    "df2 = pd.read_csv('ner/ner_dataset.csv', encoding= 'unicode_escape')\n",
    "#df2=df2[[\"Word\",\"Tag\"]]\n",
    "df_test=df2[[\"Sentence #\"]]\n",
    "df2=df2.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will create dictonaries from the dataset we prepared. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentence2word = df2.groupby('Sentence #')['Word'].apply(lambda x:x.tolist()).to_dict()\n",
    "Sentence2tag = df2.groupby('Sentence #')['Tag'].apply(lambda x:x.tolist()).to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create function which will preprocess our dataset. It will be helpful in generating the dictionaries of tokens and tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def preprocess(data,special_t):  \n",
    "    tok2idx = defaultdict(lambda: 0)\n",
    "    tok2idx[special_t] = 0 \n",
    "    #idx2tok = []\n",
    "    vocab=list(set(data))\n",
    "    for i,sp_tok in enumerate(vocab):\n",
    "        #idx2tok.append(sp_tok)\n",
    "        if sp_tok not in tok2idx.keys():\n",
    "            tok2idx[sp_tok]=i+1\n",
    "\n",
    "    return tok2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create token2id  and tag2id dictionaries using the function(preprocess) built above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2idx = preprocess(df2.Word,'<PAD>')\n",
    "tag2idx = preprocess(np.unique(df2.Tag)[:-1],'O')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also generate the id2token and id2tag with help of dictionaries created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2token= [x for x in token2idx]\n",
    "idx2tag= [x for x in tag2idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.preprocess.<locals>.<lambda>()>,\n",
       "            {'O': 0,\n",
       "             'I-org': 1,\n",
       "             'B-art': 2,\n",
       "             'B-org': 3,\n",
       "             'B-geo': 4,\n",
       "             'I-eve': 5,\n",
       "             'B-per': 6,\n",
       "             'B-eve': 7,\n",
       "             'I-gpe': 8,\n",
       "             'I-per': 9,\n",
       "             'B-gpe': 10,\n",
       "             'I-geo': 11,\n",
       "             'I-tim': 12,\n",
       "             'B-tim': 13,\n",
       "             'I-art': 14,\n",
       "             'B-nat': 15,\n",
       "             'I-nat': 16})"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the train & test set for our model. So for that we will be using 80% record of dictionaries(Sentence2word,Sentence2tag) as train set and 20% as test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size=int(len(Sentence2word.keys())*0.8)\n",
    "test_size=len(Sentence2word.keys()) - train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentence_dict={}\n",
    "for i in range(train_size):\n",
    "    \n",
    "    sentence=random.choice(list(Sentence2word.keys()))\n",
    "    train_sentence_dict[sentence] = Sentence2word[sentence]\n",
    "    \n",
    "train_tags_dict={}    \n",
    "for sentence in train_sentence_dict.keys():\n",
    "    \n",
    "    train_tags_dict[sentence] = Sentence2tag[sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence_dict={}\n",
    "test_sentences = [sentence for sentence in Sentence2word.keys() if not sentence in train_sentence_dict.keys()] \n",
    "for sentence in test_sentences:\n",
    "    \n",
    "    test_sentence_dict[sentence] = Sentence2word[sentence]\n",
    "    \n",
    "test_tags_dict={}    \n",
    "for sentence in test_sentence_dict.keys():\n",
    "    \n",
    "    test_tags_dict[sentence] = Sentence2tag[sentence]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be passing the data to the model in batches.For generating batches from the data we use following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches_generator(batch_size, sentences, tags):\n",
    "    \n",
    "    n_samples = len(sentences)\n",
    "    order = np.arange(n_samples)\n",
    "    key_list=list(sentences.keys())\n",
    "\n",
    "    n_batches = n_samples // batch_size\n",
    "    if n_samples % batch_size:\n",
    "        n_batches += 1\n",
    "\n",
    "    for k in range(n_batches):\n",
    "        batch_start = k * batch_size\n",
    "        batch_end = min((k + 1) * batch_size, n_samples)\n",
    "        current_batch_size = batch_end - batch_start\n",
    "        x_list = []\n",
    "        y_list = []\n",
    "        max_len_token = 0\n",
    "        for idx in order[batch_start: batch_end]:\n",
    "            \n",
    "            x_temp = [token2idx[token] for  i,token in enumerate(sentences[key_list[idx]])]\n",
    "            x_temp = x_temp[:-1]\n",
    "            x_list.append(x_temp)\n",
    "            \n",
    "            y_temp = [tag2idx[tag] for j,tag in enumerate(tags[key_list[idx]])]\n",
    "            y_temp = y_temp[:-1]\n",
    "            y_list.append(y_temp)\n",
    "            \n",
    "            max_len_token = max(max_len_token, len(tags[key_list[idx]]))\n",
    "            \n",
    "        # Fill in the data into numpy nd-arrays filled with padding indices.\n",
    "        x = np.ones([current_batch_size, max_len_token], dtype=np.int32) * token2idx['<PAD>']\n",
    "        y = np.ones([current_batch_size, max_len_token], dtype=np.int32) * tag2idx['O']\n",
    "        lengths = np.zeros(current_batch_size, dtype=np.int32)\n",
    "        for n in range(current_batch_size):\n",
    "            utt_len = len(x_list[n])\n",
    "            lengths[n] = utt_len\n",
    "            x[n, :utt_len] = x_list[n]\n",
    "            y[n, :utt_len] = y_list[n]\n",
    "        yield x, y, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model\n",
    "\n",
    "Let's define our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "class BiLSTMModel():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various variables/parameters for the model are as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def declare_placeholders(self):\n",
    "    self.input_batch = tf.compat.v1.placeholder(dtype=tf.int32, shape=[None, None], name='input_batch') \n",
    "    self.ground_truth_tags = tf.compat.v1.placeholder(dtype=tf.int32, shape=[None, None], name='label_batch')   \n",
    "    self.lengths = tf.compat.v1.placeholder(dtype=tf.int32, shape=[None], name='lengths') \n",
    "    \n",
    "    self.dropout_ph = tf.compat.v1.placeholder_with_default(tf.cast(1.0, tf.float32), shape=[])\n",
    "    \n",
    "    self.learning_rate_ph = tf.compat.v1.placeholder(dtype=tf.float32, shape=[]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.__declare_placeholders = classmethod(declare_placeholders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture of model will contain two LSTM Layers.Let's define that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_layers(self, vocabulary_size, embedding_dim, n_hidden_rnn, n_tags):\n",
    "    \n",
    "    initial_embedding_matrix = np.random.randn(vocabulary_size, embedding_dim) / np.sqrt(embedding_dim)\n",
    "    embedding_matrix_variable = tf.compat.v1.Variable(initial_embedding_matrix,dtype=tf.float32,name='embeddings_matrix') \n",
    "    \n",
    "    forward_cell =  tf.compat.v1.nn.rnn_cell.DropoutWrapper(tf.compat.v1.nn.rnn_cell.LSTMCell(num_units=n_hidden_rnn,use_peepholes=True,name='forward_lstm'),input_keep_prob=self.dropout_ph,output_keep_prob=self.dropout_ph)\n",
    "    backward_cell =  tf.compat.v1.nn.rnn_cell.DropoutWrapper(tf.compat.v1.nn.rnn_cell.LSTMCell(num_units=n_hidden_rnn,use_peepholes=True,name='back_lstm'),input_keep_prob=self.dropout_ph,output_keep_prob=self.dropout_ph)\n",
    "\n",
    "\n",
    "    embeddings = tf.compat.v1.nn.embedding_lookup(embedding_matrix_variable,self.input_batch) \n",
    "    \n",
    "    (rnn_output_fw, rnn_output_bw), _ = tf.compat.v1.nn.bidirectional_dynamic_rnn(forward_cell,backward_cell,inputs=embeddings,sequence_length=self.lengths,dtype=tf.float32) ######### YOUR CODE HERE #############\n",
    "    rnn_output = tf.compat.v1.concat([rnn_output_fw, rnn_output_bw], axis=2)\n",
    "   \n",
    "    self.logits = tf.compat.v1.layers.dense(rnn_output, n_tags, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.__build_layers = classmethod(build_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction function for our model is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_predictions(self):\n",
    "    \n",
    "    softmax_output = tf.nn.softmax(self.logits,name='softmax') \n",
    "    \n",
    "    self.predictions = tf.math.argmax(softmax_output,axis=-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.__compute_predictions = classmethod(compute_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for computing loss is defined as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(self, n_tags, PAD_index):\n",
    "    \n",
    "    ground_truth_tags_one_hot = tf.one_hot(self.ground_truth_tags, n_tags)\n",
    "    loss_tensor = tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(ground_truth_tags_one_hot,self.logits) \n",
    "    \n",
    "    mask = tf.cast(tf.not_equal(self.input_batch, PAD_index), tf.float32)\n",
    "\n",
    "    self.loss = tf.reduce_mean(loss_tensor*mask) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.__compute_loss = classmethod(compute_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a function which will do optimization and train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_optimization(self):\n",
    "    \n",
    "    self.optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=self.learning_rate_ph) \n",
    "    self.grads_and_vars = self.optimizer.compute_gradients(self.loss)\n",
    "    \n",
    "    clip_norm = tf.cast(1.0, tf.float32)\n",
    "    self.grads_and_vars =[ (tf.clip_by_norm(gv[0], clip_norm),gv[1]) for gv in self.grads_and_vars]  \n",
    "    \n",
    "    self.train_op = self.optimizer.apply_gradients(self.grads_and_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.__perform_optimization = classmethod(perform_optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's aggregate our model with the functions we defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(self, vocabulary_size, n_tags, embedding_dim, n_hidden_rnn, PAD_index):\n",
    "    self.__declare_placeholders()\n",
    "    self.__build_layers(vocabulary_size, embedding_dim, n_hidden_rnn, n_tags)\n",
    "    self.__compute_predictions()\n",
    "    self.__compute_loss(n_tags, PAD_index)\n",
    "    self.__perform_optimization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.__init__ = classmethod(init_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for training on a batch is defined as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_batch(self, session, x_batch, y_batch, lengths, learning_rate, dropout_keep_probability):\n",
    "    feed_dict = {self.input_batch: x_batch,\n",
    "                 self.ground_truth_tags: y_batch,\n",
    "                 self.learning_rate_ph: learning_rate,\n",
    "                 self.dropout_ph: dropout_keep_probability,\n",
    "                 self.lengths: lengths}\n",
    "    \n",
    "    session.run(self.train_op, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.train_on_batch = classmethod(train_on_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for predicting on a batch is defined as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_batch(self, session, x_batch, lengths):\n",
    "    \n",
    "\n",
    "    feed_dict = {self.input_batch: x_batch,\n",
    "                        self.lengths: lengths}\n",
    "    predictions = session.run(self.predictions,feed_dict=feed_dict)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTMModel.predict_for_batch = classmethod(predict_for_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training\n",
    "Let's run our model and train it over the dataset we prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "model = BiLSTMModel(vocabulary_size=len(token2idx),n_tags=len(tag2idx),embedding_dim=200,n_hidden_rnn=200,PAD_index=token2idx['<PAD>']) ######### YOUR CODE HERE #############\n",
    "\n",
    "batch_size = 32 \n",
    "n_epochs = 4  \n",
    "learning_rate = 0.005 \n",
    "learning_rate_decay = np.sqrt(2) \n",
    "dropout_keep_probability = 0.9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Epoch 1 of 4 --------------------\n",
      "Train data evaluation:\n",
      "processed 437864 tokens with 46665 phrases; found: 318335 phrases; correct: 2600.\n",
      "\n",
      "precision:  0.82%; recall:  5.57%; FB1:  1.42\n",
      "\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 139324 tokens with 14904 phrases; found: 101180 phrases; correct: 825.\n",
      "\n",
      "precision:  0.82%; recall:  5.54%; FB1:  1.42\n",
      "\n",
      "\n",
      "-------------------- Epoch 2 of 4 --------------------\n",
      "Train data evaluation:\n",
      "processed 437864 tokens with 46665 phrases; found: 49073 phrases; correct: 24781.\n",
      "\n",
      "precision:  50.50%; recall:  53.10%; FB1:  51.77\n",
      "\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 139324 tokens with 14904 phrases; found: 15345 phrases; correct: 7594.\n",
      "\n",
      "precision:  49.49%; recall:  50.95%; FB1:  50.21\n",
      "\n",
      "\n",
      "-------------------- Epoch 3 of 4 --------------------\n",
      "Train data evaluation:\n",
      "processed 437864 tokens with 46665 phrases; found: 48383 phrases; correct: 34996.\n",
      "\n",
      "precision:  72.33%; recall:  74.99%; FB1:  73.64\n",
      "\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 139324 tokens with 14904 phrases; found: 15056 phrases; correct: 10557.\n",
      "\n",
      "precision:  70.12%; recall:  70.83%; FB1:  70.47\n",
      "\n",
      "\n",
      "-------------------- Epoch 4 of 4 --------------------\n",
      "Train data evaluation:\n",
      "processed 437864 tokens with 46665 phrases; found: 48344 phrases; correct: 37560.\n",
      "\n",
      "precision:  77.69%; recall:  80.49%; FB1:  79.07\n",
      "\n",
      "\n",
      "Validation data evaluation:\n",
      "processed 139324 tokens with 14904 phrases; found: 15203 phrases; correct: 11131.\n",
      "\n",
      "precision:  73.22%; recall:  74.68%; FB1:  73.94\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sess = tf.compat.v1.Session()\n",
    "sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # For each epoch evaluate the model on train and validation data\n",
    "    print('-' * 20 + ' Epoch {} '.format(epoch+1) + 'of {} '.format(n_epochs) + '-' * 20)\n",
    "    print('Train data evaluation:')\n",
    "    eval_conll(model, sess, dict(list(train_sentence_dict.items())[0:20000]), dict(list(train_tags_dict.items())[0:20000]), short_report=True)\n",
    "    print('Validation data evaluation:')\n",
    "    eval_conll(model, sess, dict(list(train_sentence_dict.items())[20000:-1]), dict(list(train_tags_dict.items())[20000:-1]), short_report=True)\n",
    "    \n",
    "    # Train the model\n",
    "    for x_batch, y_batch, lengths in batches_generator(batch_size,dict(list(train_sentence_dict.items())[0:20000]) ,dict(list(train_tags_dict.items())[0:20000])):\n",
    "        model.train_on_batch(sess, x_batch, y_batch, lengths, learning_rate, dropout_keep_probability)\n",
    "        #tags_batch, tokens_batch = predict_tags(model, sess, x_batch, lengths)\n",
    "        #print(tags_batch)\n",
    "    # Decaying the learning rate\n",
    "    learning_rate = learning_rate / learning_rate_decay\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tags(model, session, token_idxs_batch, lengths):\n",
    "    \n",
    "    tag_idxs_batch = model.predict_for_batch(session, token_idxs_batch, lengths)\n",
    "    \n",
    "    tags_batch, tokens_batch = [], []\n",
    "    for tag_idxs, token_idxs in zip(tag_idxs_batch, token_idxs_batch):\n",
    "        tags, tokens = [], []\n",
    "        for tag_idx, token_idx in zip(tag_idxs, token_idxs):\n",
    "            tags.append(idx2tag[tag_idx]  )\n",
    "            tokens.append(idx2token[token_idx] )\n",
    "        tags_batch.append(tags)\n",
    "        tokens_batch.append(tokens)\n",
    "    return tags_batch, tokens_batch\n",
    "    \n",
    "    \n",
    "def eval_conll(model, session, tokens, tags, short_report=True):\n",
    "    \n",
    "    y_true, y_pred = [], []\n",
    "    for x_batch, y_batch, lengths in batches_generator(1, tokens, tags):\n",
    "        tags_batch, tokens_batch = predict_tags(model, session, x_batch, lengths)\n",
    "        if len(x_batch[0]) != len(tags_batch[0]):\n",
    "            raise Exception(\"Incorrect length of prediction for the input, \"\n",
    "                            \"expected length: %i, got: %i\" % (len(x_batch[0]), len(tags_batch[0])))\n",
    "        predicted_tags = []\n",
    "        ground_truth_tags = []\n",
    "        for gt_tag_idx, pred_tag, token in zip(y_batch[0], tags_batch[0], tokens_batch[0]): \n",
    "            if token != '<PAD>':\n",
    "                ground_truth_tags.append(idx2tag[gt_tag_idx])\n",
    "                predicted_tags.append(pred_tag)\n",
    "\n",
    "        y_true.extend(ground_truth_tags + ['O'])\n",
    "        y_pred.extend(predicted_tags + ['O'])\n",
    "        \n",
    "    \n",
    "    return precision_recall_f1(y_true, y_pred, print_results=True, short_report=short_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
